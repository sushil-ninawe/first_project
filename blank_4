
 mfcc_df = pd.DataFrame(mfcc_data.tolist(), columns=[f'mfcc_{i}' for i in range(1, num_mfcc + 1)])


import pandas as pd
from fuzzywuzzy import fuzz
from fuzzywuzzy import process

# Sample data for demonstration
data1 = {'emp_id': [1, 2, 3],
         'full_name': ['John Doe', 'Jane Smith', 'Bob Johnson']}
data2 = {'txn_id': [101, 102, 103],
         'full_name': ['Jhon Doe', 'Alice Brown', 'Bb Jhonsen']}

df_employee = pd.DataFrame(data1)
df_transaction = pd.DataFrame(data2)

# Define a function to calculate the fuzz ratio between two names
def calculate_fuzz_ratio(name1, name2):
    return fuzz.ratio(name1, name2)

# Apply the function to calculate the fuzz ratio for each combination of names
df_transaction['fuzz_ratio'] = df_transaction['full_name'].apply(lambda x: process.extractOne(x, df_employee['full_name'], scorer=calculate_fuzz_ratio)[1])

# Set a threshold for the fuzz ratio to consider a match
threshold = 80  # You can adjust this threshold based on your requirements

# Filter the transactions based on the fuzz ratio threshold
matched_transactions = df_transaction[df_transaction['fuzz_ratio'] >= threshold]

# Display the resulting dataframe with transactions belonging to employees only
print(matched_transactions[['txn_id', 'full_name']])







# pivot table logic 
# Group by 'emp_id' and 'access_type', concatenate access numbers into a string
result_df = df.groupby(['emp_id', 'access_type'])['access_number'].agg(lambda x: ' '.join(x)).reset_index()

# Pivot the DataFrame to create separate columns for each access_type
result_df = result_df.pivot(index='emp_id', columns='access_type', values='access_number').reset_index()

# Rename the columns if needed
result_df.columns = ['emp_id', 'access_number_DIV', 'access_number_FA']

# Split the concatenated strings back into lists
result_df['access_number_DIV'] = result_df['access_number_DIV'].str.split()
result_df['access_number_FA'] = result_df['access_number_FA'].str.split()

# Fill NaN values with empty lists
result_df = result_df.fillna([])

# Print the result
print(result_df)



## jacarrd similarity 


SELECT
    size(intersect(collect_set(tokenize('kitten')), collect_set(tokenize('sitting')))) /
    size(union(collect_set(tokenize('kitten')), collect_set(tokenize('sitting')))) AS jaccard_similarity;



# Define the string variable
my_string = "This is a sample string."

# Define the file path
file_path = "/path/to/save/file.txt"

# Open the file in write mode and write the string variable to it
with open(file_path, "w") as file:
    file.write(my_string)

print("String saved to file successfully.")






Title: Ingestion Pipeline Creation for EPR Data from NAS to Hadoop Data Lake

Description:
As part of this story, we aim to develop an ingestion pipeline to facilitate the seamless transfer of EPR (Enterprise Resource Planning) data generated by the IAM (Identity and Access Management) team. The EPR data is currently stored in a NAS (Network Attached Storage) location.

Objective:
- Develop an automated ingestion pipeline to pick up flat files from the designated NAS location.
- Create an EPR table within the Hadoop Data Lake on the WM_data_lake side.
- Grant appropriate access to EON (Enterprise Operations Network) and production _id.
- Identify and integrate necessary production _id for data mapping.
- Obtain requisite approvals for the pipeline implementation.

Acceptance Criteria:
1. The ingestion pipeline reliably picks up flat files from the specified NAS location.
2. The EPR table is accurately created within the designated Hadoop Data Lake, adhering to predefined schema and data governance standards.
3. Access permissions are granted to EON and production _id, ensuring data security and integrity.
4. Production _id is correctly identified and incorporated into the pipeline for data mapping purposes.
5. All necessary approvals for the pipeline implementation are obtained from relevant stakeholders.
6. Documentation detailing the pipeline architecture, data flow, and access controls is provided.

Tasks:
1. Analyze EPR data structure and requirements.
2. Design the ingestion pipeline architecture, including error handling and data validation.
3. Develop scripts or workflows for file ingestion and table creation.
4. Implement access controls for EON and production _id.
5. Identify and integrate production _id into the pipeline.
6. Initiate approval process with relevant stakeholders.
7. Test the pipeline thoroughly, ensuring reliability and data accuracy.
8. Document the pipeline implementation, including setup instructions and troubleshooting guidelines.

Dependencies:
- Access to NAS location and Hadoop Data Lake environment.
- Collaboration with IAM team for data structure and access requirements.
- Approval from data governance and security teams for access controls.

Estimation:
The estimation for completion of this story is subject to detailed analysis and may vary based on the complexity of data structures, access requirements, and approval processes. A preliminary estimate would be [provide an estimated timeframe].
