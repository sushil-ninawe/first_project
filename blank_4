
 mfcc_df = pd.DataFrame(mfcc_data.tolist(), columns=[f'mfcc_{i}' for i in range(1, num_mfcc + 1)])


import pandas as pd
from fuzzywuzzy import fuzz
from fuzzywuzzy import process

# Sample data for demonstration
data1 = {'emp_id': [1, 2, 3],
         'full_name': ['John Doe', 'Jane Smith', 'Bob Johnson']}
data2 = {'txn_id': [101, 102, 103],
         'full_name': ['Jhon Doe', 'Alice Brown', 'Bb Jhonsen']}

df_employee = pd.DataFrame(data1)
df_transaction = pd.DataFrame(data2)

# Define a function to calculate the fuzz ratio between two names
def calculate_fuzz_ratio(name1, name2):
    return fuzz.ratio(name1, name2)

# Apply the function to calculate the fuzz ratio for each combination of names
df_transaction['fuzz_ratio'] = df_transaction['full_name'].apply(lambda x: process.extractOne(x, df_employee['full_name'], scorer=calculate_fuzz_ratio)[1])

# Set a threshold for the fuzz ratio to consider a match
threshold = 80  # You can adjust this threshold based on your requirements

# Filter the transactions based on the fuzz ratio threshold
matched_transactions = df_transaction[df_transaction['fuzz_ratio'] >= threshold]

# Display the resulting dataframe with transactions belonging to employees only
print(matched_transactions[['txn_id', 'full_name']])







# pivot table logic 
# Group by 'emp_id' and 'access_type', concatenate access numbers into a string
result_df = df.groupby(['emp_id', 'access_type'])['access_number'].agg(lambda x: ' '.join(x)).reset_index()

# Pivot the DataFrame to create separate columns for each access_type
result_df = result_df.pivot(index='emp_id', columns='access_type', values='access_number').reset_index()

# Rename the columns if needed
result_df.columns = ['emp_id', 'access_number_DIV', 'access_number_FA']

# Split the concatenated strings back into lists
result_df['access_number_DIV'] = result_df['access_number_DIV'].str.split()
result_df['access_number_FA'] = result_df['access_number_FA'].str.split()

# Fill NaN values with empty lists
result_df = result_df.fillna([])

# Print the result
print(result_df)



## jacarrd similarity 


SELECT
    size(intersect(collect_set(tokenize('kitten')), collect_set(tokenize('sitting')))) /
    size(union(collect_set(tokenize('kitten')), collect_set(tokenize('sitting')))) AS jaccard_similarity;



# Define the string variable
my_string = "This is a sample string."

# Define the file path
file_path = "/path/to/save/file.txt"

# Open the file in write mode and write the string variable to it
with open(file_path, "w") as file:
    file.write(my_string)

print("String saved to file successfully.")






Title: Ingestion Pipeline Creation for EPR Data from NAS to Hadoop Data Lake

Description:
As part of this story, we aim to develop an ingestion pipeline to facilitate the seamless transfer of EPR (Enterprise Resource Planning) data generated by the IAM (Identity and Access Management) team. The EPR data is currently stored in a NAS (Network Attached Storage) location.

Objective:
- Develop an automated ingestion pipeline to pick up flat files from the designated NAS location.
- Create an EPR table within the Hadoop Data Lake on the WM_data_lake side.
- Grant appropriate access to EON (Enterprise Operations Network) and production _id.
- Identify and integrate necessary production _id for data mapping.
- Obtain requisite approvals for the pipeline implementation.

Acceptance Criteria:
1. The ingestion pipeline reliably picks up flat files from the specified NAS location.
2. The EPR table is accurately created within the designated Hadoop Data Lake, adhering to predefined schema and data governance standards.
3. Access permissions are granted to EON and production _id, ensuring data security and integrity.
4. Production _id is correctly identified and incorporated into the pipeline for data mapping purposes.
5. All necessary approvals for the pipeline implementation are obtained from relevant stakeholders.
6. Documentation detailing the pipeline architecture, data flow, and access controls is provided.

Tasks:
1. Analyze EPR data structure and requirements.
2. Design the ingestion pipeline architecture, including error handling and data validation.
3. Develop scripts or workflows for file ingestion and table creation.
4. Implement access controls for EON and production _id.
5. Identify and integrate production _id into the pipeline.
6. Initiate approval process with relevant stakeholders.
7. Test the pipeline thoroughly, ensuring reliability and data accuracy.
8. Document the pipeline implementation, including setup instructions and troubleshooting guidelines.

Dependencies:
- Access to NAS location and Hadoop Data Lake environment.
- Collaboration with IAM team for data structure and access requirements.
- Approval from data governance and security teams for access controls.

Estimation:
The estimation for completion of this story is subject to detailed analysis and may vary based on the complexity of data structures, access requirements, and approval processes. A preliminary estimate would be [provide an estimated timeframe].








Subject: Request for Meeting to Discuss Access Activity Logs

Dear [Recipient],

I hope this email finds you well. We are currently facing a challenge regarding access activity logs and believe that a collaborative discussion will be beneficial in finding a solution. Below are the key points we'd like to address:

- **Problem Statement**: 
    - We are seeking access activity logs in the form of datasets/feeds to establish that an employee with suitable access levels has engaged in access activity within the past [x number of days].
    - This will aid in reducing the volume of alerts by filtering based on activity logs.

- **Current Situation**:
    - The TOR team has assisted in identifying that the EBC team leverages TOR team's Access Level Data by querying the TOR database to verify access levels for employees.
    - However, detailed information at the employee and application granularity is not available with the TOR team.

- **Request for Collaboration**:
    - We believe that the EBC team may have maintained access activity logs at the employee and application level, which could significantly assist in reducing alert volumes.
    - We propose a meeting to discuss how we can leverage this data effectively to address our current challenge.

- **Objective of the Meeting**:
    - Understand the availability and structure of access activity logs maintained by the EBC team.
    - Explore possibilities of integrating these logs into our alert filtering mechanism.
    - Discuss any potential challenges or concerns regarding data access and privacy.

- **Proposed Meeting Details**:
    - Date: [Proposed Date]
    - Time: [Proposed Time]
    - Duration: [Estimated Duration]
    - Location/Platform: [Physical Location or Virtual Meeting Platform]

We appreciate your attention to this matter and look forward to a fruitful discussion that will benefit both teams. Please let us know your availability, and if the proposed meeting time works for you.

Thank you for your cooperation.

Warm regards,

[Your Name]
[Your Position]
[Your Contact Information]



Hyperparameter Tuning with k-Fold Cross-Validation
Purpose: K-fold cross-validation is typically used to tune the hyperparameters of a model. It involves dividing the data into k subsets (or "folds"), training the model on k-1 folds, and testing it on the remaining fold. This process is repeated k times, with each fold serving as the testing set once.
Benefit: This approach provides a robust estimate of model performance, reducing the variance that might come from a single train-test split. It helps in selecting the best set of hyperparameters and in assessing model stability.
2. Final Model Evaluation with Hold-Out Validation
Purpose: After hyperparameters are tuned using k-fold cross-validation, the final model is typically trained on the entire training set (or a large portion of it). Then, a hold-out validation set, which was not used during the k-fold cross-validation process, is used to test the final model.
Benefit: This provides an unbiased estimate of the model's performance on unseen data, simulating how it might perform in a real-world scenario. Since the hold-out set was not used during the model tuning process, it acts as a "fresh" test set.
