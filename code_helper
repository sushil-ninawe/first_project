from pyspark.sql.functions import col

# Assuming your DataFrame is df and column name is 'counter_party_name'

filtered_df = df.filter(
    (col("counter_party_name").isNotNull()) &  # Not null
    (col("counter_party_name") != "") &        # Not an empty string
    (col("counter_party_name") != "NA")        # Not 'NA'
)



# Split the column based on '-'
df[['first_part', 'second_part']] = df['col'].str.split('-', expand=True)

# Remove any leading zeros if needed
df['first_part'] = df['first_part'].str.lstrip('0')
