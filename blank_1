from datetime import datetime, timedelta

def get_first_and_last_day_of_month(date=None):
    # If no date is passed, use the current date
    if date is None:
        date = datetime.today()

    # Get the first day of the month
    first_day = date.replace(day=1)

    # Get the last day of the month by finding the first day of the next month, then subtracting one day
    if date.month == 12:  # December edge case
        last_day = date.replace(year=date.year + 1, month=1, day=1) - timedelta(days=1)
    else:
        last_day = date.replace(month=date.month + 1, day=1) - timedelta(days=1)

    return first_day, last_day


# Example usage:
first_day, last_day = get_first_and_last_day_of_month()
print("First day of the month:", first_day.strftime("%Y-%m-%d"))
print("Last day of the month:", last_day.strftime("%Y-%m-%d"))









ohyaCertainly! You can achieve the equivalent using pandas operations. Assuming you have two DataFrames named `emp_df` and `terminated_emp_df`, you can use the `merge` function to perform the equivalent of the SQL query. Here's an example:

```python
import pandas as pd

# Assuming you have DataFrames emp_df and terminated_emp_df
result_df = emp_df.merge(terminated_emp_df[['emp_id']], on='emp_id', how='left', indicator=True)
filtered_df = result_df[result_df['_merge'] == 'left_only'].drop(columns=['_merge'])

# Now filtered_df contains the result you're looking for
```

This code snippet merges the two DataFrames on the 'emp_id' column, includes an indicator column '_merge' to identify the source of each row, and then filters only the rows where the indicator is 'left_only', indicating that the 'emp_id' is not in the 'terminated_emp_df'.






Certainly! You can convert the given SQL query to Pandas operations using the `pandas` library. Assuming you have a DataFrame named `df`:

```python
import pandas as pd

# Assuming 'df' is your DataFrame
result_df = (df.assign(rower=df.sort_values('date', ascending=False)
                       .groupby('age')
                       .cumcount() + 1)
             .query('rower == 1')
             .drop(columns='rower'))
```

This Pandas code replicates the logic of the SQL query, creating a new column 'rower' based on the specified conditions and then filtering the DataFrame where 'rower' equals 1. Adjust the column names and DataFrame as needed.





import pandas as pd

# Example DataFrame with a column containing lists
df = pd.DataFrame({'col_with_lists': [[1, 2, 3], [4, 5], [6, 7, 8, 9]]})

# Explode the column containing lists
exploded_df = df['col_with_lists'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('col_with_lists')

# Reset index if needed
exploded_df.reset_index(drop=True, inplace=True)

# Display the exploded DataFrame
print(exploded_df)




flattened

import pandas as pd

# Assuming your DataFrame is named df
df['flattened_list'] = df['office list'].apply(lambda x: [item for sublist in x for item in sublist] if isinstance(x, list) and any(isinstance(i, list) for i in x) els







import pandas as pd

# Your DataFrame
data = {
    'trans_id': [1, 2, 3],
    'office_list': [['A', 'B', 'C'], [['D']], [['E', 'F'], ['G', 'H']]],
}

df = pd.DataFrame(data)

# Define a function to flatten the lists
def flatten_list(lst):
    flat_list = []
    for item in lst:
        if isinstance(item, list):
            flat_list.extend(item)
        else:
            flat_list.append(item)
    return flat_list

# Apply the function to flatten the lists in 'office_list'
df['flattened_list'] = df['office_list'].apply(flatten_list)

print(df)







SpeechBrain currently supports many conversational AI technologies, including:

- Speech Recognition
- Speaker Recognition
- Speech Separation
- Speech Enhancement
- Text-to-Speech
- Vocoding
- Spoken Language Understanding
- Speech-to-Speech Translation
- Speech Translation
- Emotion Classification
- Language Identification
- Voice Activity Detection
- Sound Classification
- Self-Supervised Learning
- Interpretabiliy
- Speech Generation
- Metric Learning
- Allignment
- Diarization
- Language Modeling
- Response Generation
- Grapheme-to-Phoneme



Title: Analyze Employee Data, Enrich with EPR, and Identify Etrade Users for Code Implementation and Volumetric Stats Collection

Story:
As a data analyst on the HR analytics team, I want to conduct a comprehensive analysis of employee data to enrich it with Enterprise Resource Planning (EPR) data and identify employees who are users of Etrade through application-level access. This analysis will enable us to gain insights into our workforce demographics, their financial activities, and compliance with company policies.

Acceptance Criteria:

Data Analysis and Enrichment:
 Retrieve employee data from the HR database.
 Extract relevant attributes such as employee ID, name, department, and job title.
 Access EPR data sources and extract financial information associated with employees.
 Enrich employee data with EPR information, linking financial data to respective employees accurately.
Identification of Etrade Users:
 Utilize application-level access logs to identify employees who have accessed Etrade.
 Filter and narrow down employees who have interacted with Etrade applications.
 Verify the identified Etrade users against the enriched employee data to ensure accuracy.
Code Implementation:
 Develop scripts or code modules to automate the analysis process.
 Ensure the code is modular, efficient, and well-documented for future maintenance.
 Integrate the code with existing data pipelines or analytics frameworks as applicable.
Volumetric Stats Collation:
 Generate volumetric statistics summarizing employee demographics, financial activities, and Etrade usage.
 Include metrics such as employee count, average transaction volume, and frequency of Etrade usage.
 Present the collated stats in a format suitable for further analysis and reporting.
Additional Considerations:

The analysis should comply with data privacy regulations and company policies regarding employee data handling.
Collaborate with IT security teams to ensure access to Etrade application logs adheres to security protocols.
Conduct thorough testing of the code implementation to validate its accuracy and reliability.
Document any dependencies or assumptions made during the analysis process.
Definition of Done:

Analysis and enrichment of employee data completed.
Etrade users identified accurately.
Code implementation tested and integrated into data workflows.
Volumetric stats collated and ready for presentation.
Documentation updated with analysis methodology and findings.




To monitor a model using the Iris dataset and capture Population Stability Index (PSI) with Evidently AI, while utilizing the Evidently UI for logging, follow these steps:

### Step 1: Setting Up the Environment
Ensure you have Python installed along with necessary libraries. You can install the required packages using pip:
```bash
pip install pandas scikit-learn evidently
```

### Step 2: Prepare the Iris Dataset
Load the Iris dataset from `scikit-learn` and split it into training and test sets.
```python
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load dataset
data = load_iris()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

# Split dataset
train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)
```

### Step 3: Train a Model
Train a simple model, such as a Decision Tree classifier.
```python
from sklearn.tree import DecisionTreeClassifier

# Train the model
model = DecisionTreeClassifier(random_state=42)
model.fit(train_df.drop(columns=['target']), train_df['target'])
```

### Step 4: Make Predictions
Generate predictions for the test set.
```python
test_df['predictions'] = model.predict(test_df.drop(columns=['target']))
```

### Step 5: Setup Evidently for Monitoring
Create an Evidently monitor to capture PSI and use the Evidently UI for logging.

#### Step 5.1: Create a Report
First, create a report to capture PSI using Evidently.
```python
from evidently.report import Report
from evidently.metrics import DataDriftTable

# Create a report
report = Report(metrics=[
    DataDriftTable()
])

# Run the report
report.run(reference_data=train_df, current_data=test_df)
report.save_html('data_drift_report.html')
```

#### Step 5.2: Monitor with Evidently
Configure Evidently for continuous monitoring using the Evidently UI.
```python
from evidently.dashboard import Dashboard
from evidently.tabs import DataDriftTab

# Create a dashboard
dashboard = Dashboard(tabs=[DataDriftTab()])
dashboard.calculate(reference_data=train_df, current_data=test_df)
dashboard.show()
```

### Step 6: Use Evidently UI for Logging
To log the monitoring results in an Evidently UI dashboard:
1. Create a directory to store your logs.
2. Set up Evidently to periodically write monitoring results to this directory.

Here's an example of setting up a basic logging mechanism:
```python
import os
import time

# Directory to store logs
log_dir = 'evidently_logs'
os.makedirs(log_dir, exist_ok=True)

# Function to log monitoring results
def log_monitoring_results(interval=60):
    while True:
        dashboard.calculate(reference_data=train_df, current_data=test_df)
        timestamp = int(time.time())
        log_path = os.path.join(log_dir, f'dashboard_{timestamp}.html')
        dashboard.save_html(log_path)
        time.sleep(interval)

# Start logging (in a real scenario, you would run this in a separate thread or process)
log_monitoring_results()
```

### Summary
This setup monitors your model using Evidently by capturing the PSI and logging the results via the Evidently UI. Adjust the monitoring interval and logging directory as needed for your specific use case. For production use, consider deploying the logging functionality in a more robust environment, such as a cloud service or a dedicated monitoring system.





from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType
from fuzzywuzzy import fuzz

def token_set_ratio_udf(s1, s2):
    return fuzz.token_set_ratio(s1, s2)

token_set_ratio = udf(token_set_ratio_udf, IntegerType())


df_with_ratio = df.withColumn("token_set_ratio", token_set_ratio(df["text1"], df["text2"]))


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Example DataFrame
data = pd.DataFrame({
    'Column1': [1, 2, 3, 4, 5],
    'Column2': [3, 4, 2, 5, 1],
    'Column3': [2, 3, 1, 4, 5]
})

# Create a figure and axis with subplots
fig, axes = plt.subplots(nrows=1, ncols=len(data.columns), figsize=(15, 5))

# Iterate through each column and plot KDE
for i, column in enumerate(data.columns):
    sns.kdeplot(data[column], ax=axes[i], shade=True)
    axes[i].set_title(f'Distribution of {column}')

# Adjust layout
plt.tight_layout()

# Save the figure to a file
plt.savefig('kde_plots.png')

# Show the plots (optional)
plt.show()





import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Example DataFrame
data = pd.DataFrame({
    'Column1': [1, 2, 3, 4, 5],
    'Column2': [3, 4, 2, 5, 1],
    'Column3': [2, 3, 1, 4, 5],
    'Column4': [5, 4, 3, 2, 1]  # Adding an extra column for illustration
})

# Calculate number of rows and columns for subplots
num_cols = len(data.columns)
num_rows = (num_cols + 1) // 2  # This will give you 2 rows if there are 4 columns or more

# Create a figure and axis with subplots
fig, axes = plt.subplots(nrows=num_rows, ncols=2, figsize=(12, 8))

# Flatten axes if there is only one row
axes = axes.flatten() if num_rows == 1 else axes

# Iterate through each column and plot KDE
for i, column in enumerate(data.columns):
    sns.kdeplot(data[column], ax=axes[i], shade=True)
    axes[i].set_title(f'Distribution of {column}')

# Adjust layout
plt.tight_layout()

# Save the figure to a file
plt.savefig('kde_plots.png')

# Show the plots (optional)
plt.show()






# Set dtypes for all columns
new_dtypes = {'col1': 'int64', 'col2': 'category', 'col3': 'float32'}
df = df.astype(new_dtypes)




export PATH=$(echo $PATH | tr ':' '\n' | grep -v "/path/to/remove" | tr '\n' ':' | sed 's/:$//')

